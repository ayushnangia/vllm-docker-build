{"commit_hash": "021f76e4f49861b2e9ea9ccff06a46d577e3c548", "commit_short": "021f76e4f498", "commit_subject": "[Perf] Refactor LoRAManager to eliminate stream syncs and redundant computations  (#6994)", "pr_url": "https://github.com/sgl-project/sglang/pull/6994", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python3 -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompt 480 --request-rate 8 --lora-name lora", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: Modal image build failed - rebuild Docker image", "error_date": "2026-01-04", "parent_commit": "777688b8929c877e4e28c2eac208d776abe4c3af"}
{"commit_hash": "09deb20deef8181a23f66c933ea74b86fee47366", "commit_short": "09deb20deef8", "commit_subject": "Optimize the memory usage of logits processor (#420)", "pr_url": "https://github.com/sgl-project/sglang/pull/420", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "33b242df303e03886835d08a583fefe979a3ee88", "human_throughput": 1598.7}
{"commit_hash": "132dad874d2e44592d03a112e4b7d63b153e8346", "commit_short": "132dad874d2e", "commit_subject": "[PD] Optimize transfer queue forward logic for dummy rank (#6922)", "pr_url": "https://github.com/sgl-project/sglang/pull/6922", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": false, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "error": "Baseline phase produced empty metrics", "error_date": "2026-01-04", "parent_commit": "60fdad7cf343333e956a3889c12956396a1516bf", "human_throughput": 1551.9}
{"commit_hash": "148254d4db8bf3bffee23710cd1acbd5711ebd1b", "commit_short": "148254d4db8b", "commit_subject": "Improve moe reduce sum kernel performance (#2705)", "pr_url": "https://github.com/sgl-project/sglang/pull/2705", "models": ["deepseek-ai/DeepSeek-V3"], "perf_command": "python -m sglang.bench_one_batch --model deepseek-ai/DeepSeek-V3 --trust-remote-code --tp 8 --batch-size 1 --input 128 --output 256", "hardware": "H100-TP8", "has_serving": false, "repo": "sglang", "error": "Model load failure (no metrics)", "error_date": "2026-01-06", "parent_commit": "a4d6d6f1ddc9f15bfa904e7e286e3f5ba4ba5a50", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "187b85b7f38496653948a2aba546d53c09ada0f3", "commit_short": "187b85b7f384", "commit_subject": "[PD] Optimize custom mem pool usage and bump mooncake version (#7393)", "pr_url": "https://github.com/sgl-project/sglang/pull/7393", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": false, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "error": "Baseline phase produced empty metrics", "error_date": "2026-01-04", "parent_commit": "ceba0ce4f661722198f6568a54ba20cf06b7e033", "human_throughput": 1529.8}
{"commit_hash": "1acca3a2c685221cdb181c2abda4f635e1ead435", "commit_short": "1acca3a2c685", "commit_subject": "FA3 speed up: skip len operation and get batch size directly from forward batch (#5969)", "pr_url": "https://github.com/sgl-project/sglang/pull/5969", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Server failed to start", "error_date": "2026-01-06", "parent_commit": "6ea1e6ac6e2fa949cebd1b4338f9bfb7036d14fe", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "1bf1cf195302fdff14a4321eb8a17831f5c2fc11", "commit_short": "1bf1cf195302", "commit_subject": "Reduce overhead when `fork(1)` (#375)", "pr_url": "https://github.com/sgl-project/sglang/pull/375", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "error": "Benchmark failed", "error_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "e822e5900b98d89d19e0a293d9ad384f4df2945a", "human_throughput": 1591.9}
{"commit_hash": "25c83fff6a80d9e3d2749f2ead122f96fdc127e9", "commit_short": "25c83fff6a80", "commit_subject": "Performing Vocabulary Parallelism for LM Head across Attention TP Groups (#5558)", "pr_url": "https://github.com/sgl-project/sglang/pull/5558", "models": ["deepseek-ai/DeepSeek-R1"], "perf_command": "python -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1 --dist-init-addr 127.0.0.1 --nnodes 2 --node-rank 0 --trust-remote-code --attention-backend fa3 --tp 16 --cuda-graph-max-bs 40 --enable-dp-attention --dp 4 --enable-dp-lm-head", "hardware": "H100-TP16", "has_serving": true, "repo": "sglang", "error": "Model load failure (no metrics)", "error_date": "2026-01-06", "parent_commit": "9f2c9568f071bd45edd98d51cc7f1ebbb4ed5e73", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "2854a5ea9fbb31165936f633ab99915dec760f8d", "commit_short": "2854a5ea9fbb", "commit_subject": "Fix the overhead due to penalizer in bench_latency (#1496)", "pr_url": "https://github.com/sgl-project/sglang/pull/1496", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: flashinfer/torch ABI mismatch - rebuild image with matching versions", "error_date": "2026-01-04", "parent_commit": "42a2d82ba71dc86ca3b6342c978db450658b750c", "human_only_success": true, "human_only_date": "2026-01-06", "human_throughput": 1446.3}
{"commit_hash": "2a413829f42b8e8433a3e7cfd91cc9cb241cfbc0", "commit_short": "2a413829f42b", "commit_subject": "Add triton version as a fused_moe_triton config search key to avoid performace decrease in different Triton version (#5955)", "pr_url": "https://github.com/sgl-project/sglang/pull/5955", "models": ["deepseek-ai/DeepSeek-V3"], "perf_command": "python -m sglang.bench_one_batch --model deepseek-ai/DeepSeek-V3 --trust-remote-code --tp 8 --batch-size 1 --input 128 --output 256", "hardware": "H100-TP8", "has_serving": false, "repo": "sglang", "error": "Model load failure (no metrics)", "error_date": "2026-01-06", "parent_commit": "d5c097a2f9ce139c70e8c08f45a05b6b44067398", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "2a754e57b052e249ed4f8572cb6f0069ba6a495e", "commit_short": "2a754e57b052", "commit_subject": "2x performance improvement for large prefill & Fix workspace conflicts (#579)", "pr_url": "https://github.com/sgl-project/sglang/pull/579", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": false, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "error": "Baseline phase produced empty metrics", "error_date": "2026-01-04", "parent_commit": "96c503eb6029d37f896e91466e23469378dfc3dc", "human_throughput": 1598.5}
{"commit_hash": "2bd18e2d767e3a0f8afb5aff427bc8e6e4d297c0", "commit_short": "2bd18e2d767e", "commit_subject": "Memory pool: Minor optimize to avoid to (#2901)", "pr_url": "https://github.com/sgl-project/sglang/pull/2901", "models": ["meta-llama/Llama-2-7b-hf"], "perf_command": "python3 -m sglang.bench_one_batch_server --model-path meta-llama/Llama-2-7b-hf --mem-fraction-static 0.8 --batch-size 48 --disable-overlap-schedule", "hardware": "H100", "has_serving": false, "repo": "sglang", "error": "Model load failure (no metrics)", "error_date": "2026-01-06", "parent_commit": "83452dbb4a19c6a2461e972eb2b64a2df9a466b8", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "2f42749184ca3679d2bb0361903f46632408f9a2", "commit_short": "2f42749184ca", "commit_subject": "Fix topk inference performance reduce (#6474)", "pr_url": "https://github.com/sgl-project/sglang/pull/6474", "models": ["deepseek-ai/DeepSeek-V3"], "perf_command": "python -m sglang.bench_serving --model deepseek-ai/DeepSeek-V3 --tp 16", "hardware": "H100-TP16", "has_serving": true, "repo": "sglang", "error": "Model load failure (no metrics)", "error_date": "2026-01-06", "parent_commit": "d8189660a9bbd4b5b5fe2526424d42c8ffcf7195", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "3212c2ad3f7e4fb473dc807b4b176020a778ed5b", "commit_short": "3212c2ad3f7e", "commit_subject": "vlm: optimize tensor transport (#6003)", "pr_url": "https://github.com/sgl-project/sglang/pull/6003", "models": ["OpenGVLab/InternVL2_5-8B"], "perf_command": "python3 -m sglang.bench_serving --backend sglang --model OpenGVLab/InternVL2_5-8B", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Server failed to start", "error_date": "2026-01-06", "parent_commit": "534756749ae4e664f762de2645a4f63ca2901bab", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "5239d79568f3b5ce55106cb3c9d9bee7cc8e7477", "commit_short": "5239d79568f3", "commit_subject": "Speedup shared expert weight construction by avoid cloning (#5188)", "pr_url": "https://github.com/sgl-project/sglang/pull/5188", "models": ["deepseek-ai/DeepSeek-V3", "deepseek-ai/DeepSeek-R1"], "perf_command": "python -m sglang.bench_serving --model deepseek-ai/DeepSeek-V3 --dataset-name random --num-prompts 100", "hardware": "H100-TP8", "has_serving": true, "repo": "sglang", "error": "Model load failure (no metrics)", "error_date": "2026-01-06", "parent_commit": "f08154193ceaa8cfcc672d9cc312784731ec8312", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "564a898ad975192b593be81387d11faf15cb1d3e", "commit_short": "564a898ad975", "commit_subject": "Optimize mem indices mangement (#619)", "pr_url": "https://github.com/sgl-project/sglang/pull/619", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: Server timeout - no benchmark output received (old SGLang version)", "error_date": "2026-01-04", "parent_commit": "5d264a90ac5154d8e368ee558337dd3dd92e720b"}
{"commit_hash": "62757db6f0f09a6dff15b1ee1ac3029602951509", "commit_short": "62757db6f0f0", "commit_subject": "Reduce the overhead when cache is disabled (#1010)", "pr_url": "https://github.com/sgl-project/sglang/pull/1010", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: flashinfer/torch ABI mismatch - rebuild image with matching versions", "error_date": "2026-01-04", "parent_commit": "73fa2d49d539fd67548b0458a365528d3e3b6edc", "human_only_success": true, "human_only_date": "2026-01-06", "human_throughput": 355.1}
{"commit_hash": "6a2941f4d037cb5fa7c927342dc7f09387c29ab0", "commit_short": "6a2941f4d037", "commit_subject": "Improve tensor parallel performance (#625)", "pr_url": "https://github.com/sgl-project/sglang/pull/625", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: Server timeout - no benchmark output received (old SGLang version)", "error_date": "2026-01-04", "parent_commit": "5ac8b80677614a9c024740e94f9a087a39eb3499"}
{"commit_hash": "6b231325b9782555eb8e1cfcf27820003a98382b", "commit_short": "6b231325b978", "commit_subject": "[PD Perf] replace Queue to FastQueue (#6649)", "pr_url": "https://github.com/sgl-project/sglang/pull/6649", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "b1c8d4e9f31953560f2db45a3b6e68099ef00c13", "human_throughput": 1555.5}
{"commit_hash": "6b7038babd562de099b583957ff19b78c4689a37", "commit_short": "6b7038babd56", "commit_subject": "Speedup warmup when DP > 1 (#4695)", "pr_url": "https://github.com/sgl-project/sglang/pull/4695", "models": ["deepseek-ai/DeepSeek-V3"], "perf_command": "python -m sglang.bench_serving --model deepseek-ai/DeepSeek-V3", "hardware": "H100-TP8", "has_serving": true, "repo": "sglang", "error": "Model load failure (no metrics)", "error_date": "2026-01-06", "parent_commit": "57eec0bfbce964e347ef2affb999e03416f22325", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "6cb00c6398126513e37c43dd975d461765fb44c7", "commit_short": "6cb00c639812", "commit_subject": "[PD] Optimize time out logic and add env var doc for mooncake (#6761)", "pr_url": "https://github.com/sgl-project/sglang/pull/6761", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "62cac2c43abb7c2d00be3b93581ab50ab1562a10", "human_throughput": 1543.5}
{"commit_hash": "6e2da5156176ed2d7fe2445b7c7316bc1650b20a", "commit_short": "6e2da5156176", "commit_subject": "Replace time.time() to time.perf_counter() for benchmarking. (#6178)", "pr_url": "https://github.com/sgl-project/sglang/pull/6178", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "e9a47f4cb58a5a2fedd7843211684b8e4db3c0c5", "human_throughput": 1558.2}
{"commit_hash": "6f560c761b2fc2f577682d0cfda62630f37a3bb0", "commit_short": "6f560c761b2f", "commit_subject": "Improve the control of streaming and improve the first token latency in streaming (#117)", "pr_url": "https://github.com/sgl-project/sglang/pull/117", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "error": "Base Docker image cd6872334e9ead684049b8fccd5f2dac9433b1b4 broken - missing torch and core dependencies (early sglang commit before proper packaging)", "error_date": "2026-01-04", "parent_commit": "cd6872334e9ead684049b8fccd5f2dac9433b1b4", "human_throughput": 1623.1}
{"commit_hash": "6fc175968c3a9fc0521948aa3636887cd6d84107", "commit_short": "6fc175968c3a", "commit_subject": "Optimize a pad operation to accelerate 25us (#5945)", "pr_url": "https://github.com/sgl-project/sglang/pull/5945", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Server failed to start", "error_date": "2026-01-06", "parent_commit": "ad506a4e6bf3d9ac12100d4648c48df76f584c4e", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "73b13e69b4207f240650c6b51eba7a7204f64939", "commit_short": "73b13e69b420", "commit_subject": "Optimize DP attn scheduling for speculative decoding (#7285)", "pr_url": "https://github.com/sgl-project/sglang/pull/7285", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "8609e637a961dd0bd17bbf7f8f81b34cb2f7863a", "human_throughput": 1503.8}
{"commit_hash": "79961afa8281f98f380d11db45c8d4b6e66a574f", "commit_short": "79961afa8281", "commit_subject": "optimize pad operations in fa3 to accelarate 100+us (#6077)", "pr_url": "https://github.com/sgl-project/sglang/pull/6077", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Server failed to start", "error_date": "2026-01-06", "parent_commit": "cfca4e0ed2cf4a97c2ee3b668f7115b59db0028a", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "7ce36068914503c3a53ad7be23ab29831fb8aa63", "commit_short": "7ce360689145", "commit_subject": "Faster overlap mode scheduler (#1738)", "pr_url": "https://github.com/sgl-project/sglang/pull/1738", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python3 -m sglang.bench_serving --model meta-llama/Llama-3.1-8B-Instruct --num-prompt 3000 --disable-radix --enable-overlap", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Server failed to start", "error_date": "2026-01-06", "parent_commit": "efb099cdee90b9ad332fcda96d89dd91ddebe072", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "880221bd3b3e56a4bc2268fe9a9f77f426accf6c", "commit_short": "880221bd3b3e", "commit_subject": "Revert \"[PD Disaggregation] replace transfer with batch transfer for better performance (#7236)\" (#7968)", "pr_url": "https://github.com/sgl-project/sglang/pull/7968", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "8f3173d0b0721acc94a39fb654eb46a4f298958d", "human_throughput": 1624.8}
{"commit_hash": "8f8f96a6217ea737c94e7429e480196319594459", "commit_short": "8f8f96a6217e", "commit_subject": "Fix the perf regression due to additional_stop_token_ids (#1773)", "pr_url": "https://github.com/sgl-project/sglang/pull/1773", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Server failed to start", "error_date": "2026-01-06", "parent_commit": "05b3bf5e8e4751cf51510198ae2e864c4b11ac2f", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "912788c095c9306daabc996fd06e59cf062a783b", "commit_short": "912788c095c9", "commit_subject": "perf: optimize local_block_table memory allocation (#6273)", "pr_url": "https://github.com/sgl-project/sglang/pull/6273", "models": ["meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"], "perf_command": "python -m sglang.bench_serving --model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --num-prompts 100", "hardware": "H100-TP8", "has_serving": true, "repo": "sglang", "error": "Model load failure (no metrics)", "error_date": "2026-01-06", "parent_commit": "0f75b907c6bf6273dbb813b9e983757dab20751f", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "9183c23eca51bf76159e81dfd6edf5770796c2d8", "commit_short": "9183c23eca51", "commit_subject": "Speed up `update_weights_from_tensor` (#2695)", "pr_url": "https://github.com/sgl-project/sglang/pull/2695", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "148254d4db8bf3bffee23710cd1acbd5711ebd1b", "human_throughput": 1567.2}
{"commit_hash": "9216b10678a036a1797e19693b0445c889016687", "commit_short": "9216b10678a0", "commit_subject": "Improve performance when running with full parallel (#394)", "pr_url": "https://github.com/sgl-project/sglang/pull/394", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "da19434c2f3cbe4f367f84993da0bcbd84efb6ba", "human_throughput": 1562.4}
{"commit_hash": "9c064bf78af8558dbc50fbd809f65dcafd6fd965", "commit_short": "9c064bf78af8", "commit_subject": "[LoRA, Performance] Speedup multi-LoRA serving - Step 1 (#1587)", "pr_url": "https://github.com/sgl-project/sglang/pull/1587", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: flashinfer/torch ABI mismatch - rebuild image with matching versions", "error_date": "2026-01-04", "parent_commit": "58d1082e392cabbf26c404cb7ec18e4cb51b99e9", "human_only_success": true, "human_only_date": "2026-01-06", "human_throughput": 1403.0}
{"commit_hash": "9c088829ee2a28263f36d0814fde448c6090b5bc", "commit_short": "9c088829ee2a", "commit_subject": "Revert \"Use device_id in dist init to reduce NCCL communicator warmup & creation overhead\" (#5786)", "pr_url": "https://github.com/sgl-project/sglang/pull/5786", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "005aad32ad45ce27d73fd39aa1f7e9ba5d8ebb8f", "human_throughput": 1568.2}
{"commit_hash": "9c745d078e29e153a64300bd07636c7c9c1c42d5", "commit_short": "9c745d078e29", "commit_subject": "[Performance] Update xgrammar-related constrained decoding (#2056)", "pr_url": "https://github.com/sgl-project/sglang/pull/2056", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Server failed to start", "error_date": "2026-01-06", "parent_commit": "ebaa2f31996e80e4128b832d70f29f288b59944e", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "a191a0e47c2f0b0c8aed28080b9cb78624365e92", "commit_short": "a191a0e47c2f", "commit_subject": "Improve performance of two batch overlap in some imbalanced cases (#6593)", "pr_url": "https://github.com/sgl-project/sglang/pull/6593", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "8c7279c24e535681478188967b3007916b87b3d0", "human_throughput": 1593.4}
{"commit_hash": "a37e1247c183cff86a18f2ed1a075e40704b1c5e", "commit_short": "a37e1247c183", "commit_subject": "[Multimodal][Perf] Use `pybase64` instead of `base64` (#7724)", "pr_url": "https://github.com/sgl-project/sglang/pull/7724", "models": ["Qwen/Qwen2.5-VL-7B-Instruct"], "perf_command": "python3 -m sglang.bench_serving --backend sglang --model Qwen/Qwen2.5-VL-7B-Instruct --dataset-name mmmu --request-rate 10 --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Server failed to start", "error_date": "2026-01-06", "parent_commit": "136c6e0431c2067c3a2a98ad2c77fc89a9cb98e7", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "a99801e0750f41553fedd02e36f58d835c4d4bd6", "commit_short": "a99801e0750f", "commit_subject": "[Performance][PD Disaggregation] optimize TokenToKVPoolAllocator by sorting free pages (#8133)", "pr_url": "https://github.com/sgl-project/sglang/pull/8133", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "4c605235aa832f259e148dfbdce08d9e471b5099", "human_throughput": 1534.2}
{"commit_hash": "ab4a83b25909aa98330b838a224e4fe5c943e483", "commit_short": "ab4a83b25909", "commit_subject": "Optimize schedule (#1339)", "pr_url": "https://github.com/sgl-project/sglang/pull/1339", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: flashinfer/torch ABI mismatch - rebuild image with matching versions", "error_date": "2026-01-04", "parent_commit": "62f15eea5a0b4266cdae965d0337fd33f6673736", "human_only_success": true, "human_only_date": "2026-01-06", "human_throughput": 1428.5}
{"commit_hash": "ac971ff633de330de3ded7f7475caaf7cd5bbdcd", "commit_short": "ac971ff633de", "commit_subject": "perf: reduce ttft and itl with stream_interval 1 (#658)", "pr_url": "https://github.com/sgl-project/sglang/pull/658", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Server failed to start - outlines version incompatibility", "error_date": "2026-01-06", "parent_commit": "e1792cca2491af86f29782a3b83533a6566ac75b", "human_only_success": false, "human_only_date": "2026-01-06", "dockerfile_fixes": {"outlines": ">=0.0.44,<0.1.0", "note": "Old sglang v0.1.21 uses outlines.fsm.guide API removed in outlines 0.1.0+"}}
{"commit_hash": "b170930534acbb9c1619a3c83670a839ceee763a", "commit_short": "b170930534ac", "commit_subject": "feat: radix tree code optimize (#1697)", "pr_url": "https://github.com/sgl-project/sglang/pull/1697", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: flashinfer/torch ABI mismatch - rebuild image with matching versions", "error_date": "2026-01-04", "parent_commit": "5ab20cceba227479bf5088a3fc95b1b4fe0ac3a9", "human_only_success": true, "human_only_date": "2026-01-06", "human_throughput": 661.8}
{"commit_hash": "b1e5a33ae337d20e35e966b8d82a02a913d32689", "commit_short": "b1e5a33ae337", "commit_subject": "Eliminate stream sync to speed up LoRA batch init  (#6960)", "pr_url": "https://github.com/sgl-project/sglang/pull/6960", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "9d5fa68b903d295d2b39201d54905c6801f60f7f", "human_throughput": 1593.6}
{"commit_hash": "b77a02cdfdb4cd58be3ebc6a66d076832c309cfc", "commit_short": "b77a02cdfdb4", "commit_subject": "[Performance] Support both xgrammar and outlines for constrained decoding (#1752)", "pr_url": "https://github.com/sgl-project/sglang/pull/1752", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Server failed to start", "error_date": "2026-01-06", "parent_commit": "30643fed7f92be32540dfcdf9e4310e477ce0f6d", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "bb3a3b6675b1844a13ebe368ad693f3dc75b315b", "commit_short": "bb3a3b6675b1", "commit_subject": "Support Faster JSON decoding for llava (#137)", "pr_url": "https://github.com/sgl-project/sglang/pull/137", "models": ["llava-hf/llava-1.5-7b-hf", "llava-hf/llava-1.5-13b-hf"], "perf_command": "python -m sglang.bench_serving --model llava-hf/llava-1.5-7b-hf --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Server failed to start", "error_date": "2026-01-06", "parent_commit": "45d6592d4053fe8b2b8dc9440f64c900de040d09", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "c087ddd6865a52634326a05af66429cb5531cd16", "commit_short": "c087ddd6865a", "commit_subject": "Refine pre_reorder_triton_kernel slightly to improve performance (#6627)", "pr_url": "https://github.com/sgl-project/sglang/pull/6627", "models": ["deepseek-ai/DeepSeek-V3"], "perf_command": "python -m sglang.bench_one_batch --model deepseek-ai/DeepSeek-V3 --trust-remote-code --tp 8 --batch-size 1 --input 128 --output 256", "hardware": "H100-TP8", "has_serving": false, "repo": "sglang", "error": "Model load failure (no metrics)", "error_date": "2026-01-06", "parent_commit": "f4a8987f6904e4909adb473c52b443a62ba5a4b5", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "c2f212d672ccaf8a1e5ef09099e981d943600b14", "commit_short": "c2f212d672cc", "commit_subject": "optimize MiniMax-Text-01 lightning_attn_decode triton (#2966)", "pr_url": "https://github.com/sgl-project/sglang/pull/2966", "models": ["MiniMaxAI/MiniMax-Text-01"], "perf_command": "python -m sglang.bench_serving --model MiniMaxAI/MiniMax-Text-01 --dataset-name random --num-prompts 100", "hardware": "H100-TP8", "has_serving": true, "repo": "sglang", "error": "Model load failure (no metrics)", "error_date": "2026-01-06", "parent_commit": "e2cdc8a5b5c6a6d5a68e39d8c3e2a0c46248a2d2", "human_only_success": false, "human_only_date": "2026-01-06"}
{"commit_hash": "c98e84c21e4313d7d307425ca43e61753a53a9f7", "commit_short": "c98e84c21e43", "commit_subject": "[Minor, Performance] Use torch.argmax for greedy sampling (#1589)", "pr_url": "https://github.com/sgl-project/sglang/pull/1589", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: flashinfer/torch ABI mismatch - rebuild image with matching versions", "error_date": "2026-01-04", "parent_commit": "9c064bf78af8558dbc50fbd809f65dcafd6fd965", "human_only_success": true, "human_only_date": "2026-01-06", "human_throughput": 624.8}
{"commit_hash": "d1112d8548eb13c842900b3a8d622345f9737759", "commit_short": "d1112d8548eb", "commit_subject": "Add endpoint for file support, purely to speed up processing of input_embeds. (#2797)", "pr_url": "https://github.com/sgl-project/sglang/pull/2797", "models": ["google/gemma-2-2b"], "perf_command": "python -m sglang.bench_serving --model google/gemma-2-2b --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "48efec7b052354865aa2f0605a5bf778721f3cbb", "human_throughput": 3127.8}
{"commit_hash": "da47621ccc4f8e8381f3249257489d5fe32aff1b", "commit_short": "da47621ccc4f", "commit_subject": "Minor speedup topk postprocessing (#7058)", "pr_url": "https://github.com/sgl-project/sglang/pull/7058", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: Modal image build failed - rebuild Docker image", "error_date": "2026-01-04", "parent_commit": "22a6b9fc051154347b6eb5064d2f6ef9b4dba471"}
{"commit_hash": "dc67d9769382cf83b3e2644a4366d6473445a6c6", "commit_short": "dc67d9769382", "commit_subject": "misc: speedup load safetensors (#1319)", "pr_url": "https://github.com/sgl-project/sglang/pull/1319", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python3 -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 5000", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: flashinfer/torch ABI mismatch - rebuild image with matching versions", "error_date": "2026-01-04", "parent_commit": "1e495e08470b6dc56645081f644831e0c620dfa5", "human_only_success": true, "human_only_date": "2026-01-06", "human_throughput": 653.3}
{"commit_hash": "dd1012fcbe2a1fb36c44e10c16f8d0bcd8e9da25", "commit_short": "dd1012fcbe2a", "commit_subject": "[PD] Fix potential perf spike caused by tracker gc and optimize doc (#6764)", "pr_url": "https://github.com/sgl-project/sglang/pull/6764", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "44aab7f91c350b1c6ecb77a7a34efb98af106cb5", "human_throughput": 1553.6}
{"commit_hash": "ddcf9fe3beacd8aed573c711942194dd02350da4", "commit_short": "ddcf9fe3beac", "commit_subject": "Optimize triton attention custom mask (#3731)", "pr_url": "https://github.com/sgl-project/sglang/pull/3731", "models": ["meta-llama/Llama-2-7b-chat-hf"], "perf_command": "python3 -m sglang.bench_serving --backend sglang --model meta-llama/Llama-2-7b-chat-hf --speculative-algo EAGLE", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Model load failure (no metrics)", "error_date": "2026-01-06", "parent_commit": "6252ade98571c3374d7e7df3430a2bfbddfc5eb3", "human_only_success": false, "human_only_date": "2026-01-06", "skip": true, "skip_reason": "Requires EAGLE speculative decoding setup - can't run with generic bench_serving command"}
{"commit_hash": "df7f61ee7d235936e6663f07813d7c03c4ec1603", "commit_short": "df7f61ee7d23", "commit_subject": "Speed up rebalancing when using non-static dispatch algorithms (#6812)", "pr_url": "https://github.com/sgl-project/sglang/pull/6812", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": true, "human_only_date": "2026-01-04", "baseline_only_success": true, "baseline_only_date": "2026-01-04", "agent_only_success": true, "agent_only_date": "2026-01-04", "parent_commit": "ef21729c1d8fdd9575cb2c8aaea96c94481c10fa", "human_throughput": 1547.6}
{"commit_hash": "e3ec6bf4b65a50e26e936a96adc7acc618292002", "commit_short": "e3ec6bf4b65a", "commit_subject": "Minor speed up block_quant_dequant (#6814)", "pr_url": "https://github.com/sgl-project/sglang/pull/6814", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: Modal image build failed - rebuild Docker image", "error_date": "2026-01-04", "parent_commit": "b04df75acdda5b99999c02820e64b5b005c07159"}
{"commit_hash": "e5db40dcbce67157e005f524bf6a5bea7dcb7f34", "commit_short": "e5db40dcbce6", "commit_subject": "ORJson. Faster Json serialization (#1694)", "pr_url": "https://github.com/sgl-project/sglang/pull/1694", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100", "hardware": "H100", "has_serving": true, "repo": "sglang", "error": "Docker: flashinfer/torch ABI mismatch - rebuild image with matching versions", "error_date": "2026-01-04", "parent_commit": "b170930534acbb9c1619a3c83670a839ceee763a", "human_only_success": true, "human_only_date": "2026-01-06", "human_throughput": 652.8}
{"commit_hash": "e822e5900b98d89d19e0a293d9ad384f4df2945a", "commit_short": "e822e5900b98", "commit_subject": "Optimize radix tree matching (#364)", "pr_url": "https://github.com/sgl-project/sglang/pull/364", "models": ["meta-llama/Llama-2-13b-chat-hf"], "perf_command": "python -m sglang.bench_serving --backend sglang --model meta-llama/Llama-2-13b-chat-hf --num-prompts 1000 --request-rate 128", "hardware": "H100", "has_serving": true, "repo": "sglang", "human_only_success": false, "human_only_date": "2026-01-04", "error": "Benchmark script path error - empty metrics", "error_date": "2026-01-04", "parent_commit": "ca4f1ab89c0c9bdd80fdfabcec52968fbde108bb"}
{"commit_hash": "93470a14116a60fe5dd43f0599206e8ccabdc211", "commit_short": "93470a14116a", "commit_subject": "Refactor and Optimize FA3 Code (#5090)", "pr_url": "https://github.com/sgl-project/sglang/pull/5090", "models": ["meta-llama/Llama-3.1-8B-Instruct"], "perf_command": "python3 -m sglang.bench_serving --backend sglang --model meta-llama/Llama-3.1-8B-Instruct", "hardware": "H200", "has_serving": true, "repo": "sglang", "parent_commit": "db452760e5b2378efd06b1ceb9385d2eeb6d217c"}
