{"commit": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc", "commit_date": "2023-02-23T09:31:55+00:00", "Dockerfile": null}
{"commit": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd", "commit_date": "2023-04-01T00:51:08+08:00", "Dockerfile": null}
{"commit": "897cb2ae28e93de1b22ecfbffcccfb9493f8f4d9", "commit_date": "2023-04-02T00:30:17-07:00", "Dockerfile": null}
{"commit": "0f40557af6141ced118b81f2a04e651a0c6c9dbd", "commit_date": "2023-04-07T17:45:07-07:00", "Dockerfile": null}
{"commit": "ec3b5ce9ccb4262194a16a8b1c31ffd6b3b824b9", "commit_date": "2023-10-13T09:59:07-07:00", "Dockerfile": null}
{"commit": "21d93c140d0a97af5f0c59e660cf04bd417fd424", "commit_date": "2023-12-13T23:55:07-08:00", "Dockerfile": "FROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# image to build pytorch extensions\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# copy input files\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY requirements.txt requirements.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm/__init__.py vllm/__init__.py\n\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nRUN python3 setup.py build_ext --inplace\n\n# image to run unit testing suite\nFROM dev AS test\n\n# copy pytorch extensions separately to avoid having to rebuild\n# when python code changes\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY tests tests\nCOPY vllm vllm\n\nENTRYPOINT [\"python3\", \"-m\", \"pytest\", \"tests\"]\n\n# use CUDA base as CUDA runtime dependencies are already installed via pip\nFROM nvidia/cuda:12.1.0-base-ubuntu22.04 AS vllm-base\n\n# libnccl required for ray\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\nFROM vllm-base AS vllm\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY vllm vllm\n\nEXPOSE 8000\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.api_server\"]\n\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate\n\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY vllm vllm\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n\n"}
{"commit": "fd4ea8ef5c17a8b991107402a414f6ed355d854d", "commit_date": "2024-01-03T11:30:22-08:00", "Dockerfile": "FROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# image to build pytorch extensions\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# copy input files\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY requirements.txt requirements.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm/__init__.py vllm/__init__.py\n\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nRUN python3 setup.py build_ext --inplace\n\n# image to run unit testing suite\nFROM dev AS test\n\n# copy pytorch extensions separately to avoid having to rebuild\n# when python code changes\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY tests tests\nCOPY vllm vllm\n\nENTRYPOINT [\"python3\", \"-m\", \"pytest\", \"tests\"]\n\n# use CUDA base as CUDA runtime dependencies are already installed via pip\nFROM nvidia/cuda:12.1.0-base-ubuntu22.04 AS vllm-base\n\n# libnccl required for ray\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\nFROM vllm-base AS vllm\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY vllm vllm\n\nEXPOSE 8000\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.api_server\"]\n\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate\n\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY vllm vllm\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n\n"}
{"commit": "f8ecb84c0283a7f1ba02ee732c9f044f8f9d36ee", "commit_date": "2024-01-27T17:46:56-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n#################### BASE BUILD IMAGE ####################\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n#################### BASE BUILD IMAGE ####################\n\n\n#################### EXTENSION BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# copy input files\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY requirements.txt requirements.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm/__init__.py vllm/__init__.py\n\n# cuda arch list used by torch\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nRUN python3 setup.py build_ext --inplace\n#################### EXTENSION Build IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\nFROM dev AS test\n\n# copy pytorch extensions separately to avoid having to rebuild\n# when python code changes\nWORKDIR /vllm-workspace\n# ADD is used to preserve directory structure\nADD . /vllm-workspace/\nCOPY --from=build /workspace/vllm/*.so /vllm-workspace/vllm/\n# ignore build dependencies installation because we are using pre-complied extensions\nRUN rm pyproject.toml\nRUN --mount=type=cache,target=/root/.cache/pip VLLM_USE_PRECOMPILED=1 pip install . --verbose\n#################### TEST IMAGE ####################\n\n\n#################### RUNTIME BASE IMAGE ####################\n# use CUDA base as CUDA runtime dependencies are already installed via pip\nFROM nvidia/cuda:12.1.0-base-ubuntu22.04 AS vllm-base\n\n# libnccl required for ray\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n#################### RUNTIME BASE IMAGE ####################\n\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate\n\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY vllm vllm\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "9474e89ba4ecae253b585eb6b3e1d85f4e108f01", "commit_date": "2024-03-20T00:11:11-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n#################### BASE BUILD IMAGE ####################\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.1/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n#################### BASE BUILD IMAGE ####################\n\n\n#################### EXTENSION BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# copy input files\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements.txt requirements.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm/__init__.py vllm/__init__.py\n\n# cuda arch list used by torch\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nRUN python3 setup.py build_ext --inplace\n#################### EXTENSION Build IMAGE ####################\n\n#################### FLASH_ATTENTION Build IMAGE ####################\nFROM dev as flash-attn-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# flash attention version\nARG flash_attn_version=v2.5.6\nENV FLASH_ATTN_VERSION=${flash_attn_version}\n\nWORKDIR /usr/src/flash-attention-v2\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel flash-attn==${FLASH_ATTN_VERSION} \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### FLASH_ATTENTION Build IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\nFROM dev AS test\n\n# copy pytorch extensions separately to avoid having to rebuild\n# when python code changes\nWORKDIR /vllm-workspace\n# ADD is used to preserve directory structure\nADD . /vllm-workspace/\nCOPY --from=build /workspace/vllm/*.so /vllm-workspace/vllm/\n# Install flash attention (from pre-built wheel)\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n# ignore build dependencies installation because we are using pre-complied extensions\nRUN rm pyproject.toml\nRUN --mount=type=cache,target=/root/.cache/pip VLLM_USE_PRECOMPILED=1 pip install . --verbose\n#################### TEST IMAGE ####################\n\n\n#################### RUNTIME BASE IMAGE ####################\n# We used base cuda image because pytorch installs its own cuda libraries.\n# However cupy depends on cuda libraries so we had to switch to the runtime image\n# In the future it would be nice to get a container with pytorch and cuda without duplicating cuda\nFROM nvidia/cuda:12.1.0-runtime-ubuntu22.04 AS vllm-base\n\n# libnccl required for ray\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# Install flash attention (from pre-built wheel)\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n\n#################### RUNTIME BASE IMAGE ####################\n\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY vllm vllm\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "cf2f084d56a1293cb08da2393984cdc7685ac019", "commit_date": "2024-03-22T12:28:14-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n#################### BASE BUILD IMAGE ####################\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.1/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n#################### BASE BUILD IMAGE ####################\n\n\n#################### EXTENSION BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# copy input files\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements.txt requirements.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm/__init__.py vllm/__init__.py\n\n# cuda arch list used by torch\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nRUN python3 setup.py build_ext --inplace\n#################### EXTENSION Build IMAGE ####################\n\n#################### FLASH_ATTENTION Build IMAGE ####################\nFROM dev as flash-attn-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# flash attention version\nARG flash_attn_version=v2.5.6\nENV FLASH_ATTN_VERSION=${flash_attn_version}\n\nWORKDIR /usr/src/flash-attention-v2\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel flash-attn==${FLASH_ATTN_VERSION} \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### FLASH_ATTENTION Build IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\nFROM dev AS test\n\n# copy pytorch extensions separately to avoid having to rebuild\n# when python code changes\nWORKDIR /vllm-workspace\n# ADD is used to preserve directory structure\nADD . /vllm-workspace/\nCOPY --from=build /workspace/vllm/*.so /vllm-workspace/vllm/\n# Install flash attention (from pre-built wheel)\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n# ignore build dependencies installation because we are using pre-complied extensions\nRUN rm pyproject.toml\nRUN --mount=type=cache,target=/root/.cache/pip VLLM_USE_PRECOMPILED=1 pip install . --verbose\n#################### TEST IMAGE ####################\n\n\n#################### RUNTIME BASE IMAGE ####################\n# We used base cuda image because pytorch installs its own cuda libraries.\n# However cupy depends on cuda libraries so we had to switch to the runtime image\n# In the future it would be nice to get a container with pytorch and cuda without duplicating cuda\nFROM nvidia/cuda:12.1.0-runtime-ubuntu22.04 AS vllm-base\n\n# libnccl required for ray\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# Install flash attention (from pre-built wheel)\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n\n#################### RUNTIME BASE IMAGE ####################\n\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY vllm vllm\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56", "commit_date": "2024-03-22T13:44:12-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n#################### BASE BUILD IMAGE ####################\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.1/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n#################### BASE BUILD IMAGE ####################\n\n\n#################### EXTENSION BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# copy input files\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements.txt requirements.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm/__init__.py vllm/__init__.py\n\n# cuda arch list used by torch\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nRUN python3 setup.py build_ext --inplace\n#################### EXTENSION Build IMAGE ####################\n\n#################### FLASH_ATTENTION Build IMAGE ####################\nFROM dev as flash-attn-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# flash attention version\nARG flash_attn_version=v2.5.6\nENV FLASH_ATTN_VERSION=${flash_attn_version}\n\nWORKDIR /usr/src/flash-attention-v2\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel flash-attn==${FLASH_ATTN_VERSION} \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### FLASH_ATTENTION Build IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\nFROM dev AS test\n\n# copy pytorch extensions separately to avoid having to rebuild\n# when python code changes\nWORKDIR /vllm-workspace\n# ADD is used to preserve directory structure\nADD . /vllm-workspace/\nCOPY --from=build /workspace/vllm/*.so /vllm-workspace/vllm/\n# Install flash attention (from pre-built wheel)\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n# ignore build dependencies installation because we are using pre-complied extensions\nRUN rm pyproject.toml\nRUN --mount=type=cache,target=/root/.cache/pip VLLM_USE_PRECOMPILED=1 pip install . --verbose\n#################### TEST IMAGE ####################\n\n\n#################### RUNTIME BASE IMAGE ####################\n# We used base cuda image because pytorch installs its own cuda libraries.\n# However cupy depends on cuda libraries so we had to switch to the runtime image\n# In the future it would be nice to get a container with pytorch and cuda without duplicating cuda\nFROM nvidia/cuda:12.1.0-runtime-ubuntu22.04 AS vllm-base\n\n# libnccl required for ray\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# Install flash attention (from pre-built wheel)\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n\n#################### RUNTIME BASE IMAGE ####################\n\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY vllm vllm\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "3a243095e5e7b655b63ab08fbd5936cb40850415", "commit_date": "2024-03-25T16:03:02-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n#################### BASE BUILD IMAGE ####################\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.1/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n#################### BASE BUILD IMAGE ####################\n\n\n#################### EXTENSION BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# copy input files\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements.txt requirements.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm/__init__.py vllm/__init__.py\n\n# cuda arch list used by torch\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nRUN python3 setup.py build_ext --inplace\n#################### EXTENSION Build IMAGE ####################\n\n#################### FLASH_ATTENTION Build IMAGE ####################\nFROM dev as flash-attn-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# flash attention version\nARG flash_attn_version=v2.5.6\nENV FLASH_ATTN_VERSION=${flash_attn_version}\n\nWORKDIR /usr/src/flash-attention-v2\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel flash-attn==${FLASH_ATTN_VERSION} \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### FLASH_ATTENTION Build IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\nFROM dev AS test\n\n# copy pytorch extensions separately to avoid having to rebuild\n# when python code changes\nWORKDIR /vllm-workspace\n# ADD is used to preserve directory structure\nADD . /vllm-workspace/\nCOPY --from=build /workspace/vllm/*.so /vllm-workspace/vllm/\n# Install flash attention (from pre-built wheel)\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n# ignore build dependencies installation because we are using pre-complied extensions\nRUN rm pyproject.toml\nRUN --mount=type=cache,target=/root/.cache/pip VLLM_USE_PRECOMPILED=1 pip install . --verbose\n#################### TEST IMAGE ####################\n\n\n#################### RUNTIME BASE IMAGE ####################\n# We used base cuda image because pytorch installs its own cuda libraries.\n# However cupy depends on cuda libraries so we had to switch to the runtime image\n# In the future it would be nice to get a container with pytorch and cuda without duplicating cuda\nFROM nvidia/cuda:12.1.0-runtime-ubuntu22.04 AS vllm-base\n\n# libnccl required for ray\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# Install flash attention (from pre-built wheel)\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n\n#################### RUNTIME BASE IMAGE ####################\n\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY vllm vllm\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "b6d103542c654fb63013a1e45a586d654ae36a2a", "commit_date": "2024-03-30T14:26:38-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n#################### BASE BUILD IMAGE ####################\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.1/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n#################### BASE BUILD IMAGE ####################\n\n\n#################### EXTENSION BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# copy input files\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements.txt requirements.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm/__init__.py vllm/__init__.py\n\n# cuda arch list used by torch\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    python3 setup.py build_ext --inplace\n#################### EXTENSION Build IMAGE ####################\n\n#################### FLASH_ATTENTION Build IMAGE ####################\nFROM dev as flash-attn-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# flash attention version\nARG flash_attn_version=v2.5.6\nENV FLASH_ATTN_VERSION=${flash_attn_version}\n\nWORKDIR /usr/src/flash-attention-v2\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel flash-attn==${FLASH_ATTN_VERSION} \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### FLASH_ATTENTION Build IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\nFROM dev AS test\n\n# copy pytorch extensions separately to avoid having to rebuild\n# when python code changes\nWORKDIR /vllm-workspace\n# ADD is used to preserve directory structure\nADD . /vllm-workspace/\nCOPY --from=build /workspace/vllm/*.so /vllm-workspace/vllm/\n# Install flash attention (from pre-built wheel)\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n# ignore build dependencies installation because we are using pre-complied extensions\nRUN rm pyproject.toml\nRUN --mount=type=cache,target=/root/.cache/pip VLLM_USE_PRECOMPILED=1 pip install . --verbose\n#################### TEST IMAGE ####################\n\n\n#################### RUNTIME BASE IMAGE ####################\n# We used base cuda image because pytorch installs its own cuda libraries.\n# However pynccl depends on cuda libraries so we had to switch to the runtime image\n# In the future it would be nice to get a container with pytorch and cuda without duplicating cuda\nFROM nvidia/cuda:12.1.0-runtime-ubuntu22.04 AS vllm-base\n\n# libnccl required for ray\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# Install flash attention (from pre-built wheel)\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n\n#################### RUNTIME BASE IMAGE ####################\n\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY vllm vllm\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "2f1928354903ae0c6edfe76cc90081eb513ead2c", "commit_date": "2024-04-06T19:14:06-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.1/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 setup.py bdist_wheel --dist-dir=dist\n\n# the `vllm_nccl` package must be installed from source distribution\n# pip is too smart to store a wheel in the cache, and other CI jobs\n# will directly use the wheel from the cache, which is not what we want.\n# we need to remove it manually\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip cache remove vllm_nccl*\n#################### EXTENSION Build IMAGE ####################\n\n#################### FLASH_ATTENTION Build IMAGE ####################\nFROM dev as flash-attn-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# flash attention version\nARG flash_attn_version=v2.5.6\nENV FLASH_ATTN_VERSION=${flash_attn_version}\n\nWORKDIR /usr/src/flash-attention-v2\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel flash-attn==${FLASH_ATTN_VERSION} \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### FLASH_ATTENTION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.1.0-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.1/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "ad8d696a99ca1eee19f1404e16e8e82df592ff85", "commit_date": "2024-04-22T21:11:06+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.1/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 setup.py bdist_wheel --dist-dir=dist\n\n# the `vllm_nccl` package must be installed from source distribution\n# pip is too smart to store a wheel in the cache, and other CI jobs\n# will directly use the wheel from the cache, which is not what we want.\n# we need to remove it manually\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip cache remove vllm_nccl*\n#################### EXTENSION Build IMAGE ####################\n\n#################### FLASH_ATTENTION Build IMAGE ####################\nFROM dev as flash-attn-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# flash attention version\nARG flash_attn_version=v2.5.6\nENV FLASH_ATTN_VERSION=${flash_attn_version}\n\nWORKDIR /usr/src/flash-attention-v2\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel flash-attn==${FLASH_ATTN_VERSION} \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### FLASH_ATTENTION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.1.0-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.1/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3", "commit_date": "2024-05-04T11:45:16-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.1/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 setup.py bdist_wheel --dist-dir=dist\n\n# the `vllm_nccl` package must be installed from source distribution\n# pip is too smart to store a wheel in the cache, and other CI jobs\n# will directly use the wheel from the cache, which is not what we want.\n# we need to remove it manually\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip cache remove vllm_nccl*\n#################### EXTENSION Build IMAGE ####################\n\n#################### FLASH_ATTENTION Build IMAGE ####################\nFROM dev as flash-attn-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# flash attention version\nARG flash_attn_version=v2.5.8\nENV FLASH_ATTN_VERSION=${flash_attn_version}\n\nWORKDIR /usr/src/flash-attention-v2\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel flash-attn==${FLASH_ATTN_VERSION} \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### FLASH_ATTENTION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.1.0-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.1/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "d7740ea4dcee4ab75d7d6eef723f33cae957b288", "commit_date": "2024-05-08T08:42:28-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 setup.py bdist_wheel --dist-dir=dist\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n# the `vllm_nccl` package must be installed from source distribution\n# pip is too smart to store a wheel in the cache, and other CI jobs\n# will directly use the wheel from the cache, which is not what we want.\n# we need to remove it manually\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip cache remove vllm_nccl*\n#################### EXTENSION Build IMAGE ####################\n\n#################### FLASH_ATTENTION Build IMAGE ####################\nFROM dev as flash-attn-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# flash attention version\nARG flash_attn_version=v2.5.8\nENV FLASH_ATTN_VERSION=${flash_attn_version}\n\nWORKDIR /usr/src/flash-attention-v2\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel flash-attn==${FLASH_ATTN_VERSION} \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### FLASH_ATTENTION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.4.1-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "ad932a221d2a4c1e6355021bb9e9c47f7a179e51", "commit_date": "2024-05-08T10:33:18-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 setup.py bdist_wheel --dist-dir=dist\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n# the `vllm_nccl` package must be installed from source distribution\n# pip is too smart to store a wheel in the cache, and other CI jobs\n# will directly use the wheel from the cache, which is not what we want.\n# we need to remove it manually\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip cache remove vllm_nccl*\n#################### EXTENSION Build IMAGE ####################\n\n#################### FLASH_ATTENTION Build IMAGE ####################\nFROM dev as flash-attn-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# flash attention version\nARG flash_attn_version=v2.5.8\nENV FLASH_ATTN_VERSION=${flash_attn_version}\n\nWORKDIR /usr/src/flash-attention-v2\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel flash-attn==${FLASH_ATTN_VERSION} \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### FLASH_ATTENTION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.4.1-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "379da6dcb5f5d062d0452b2fc23291e5113dcf04", "commit_date": "2024-05-09T16:38:07-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 setup.py bdist_wheel --dist-dir=dist\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n# the `vllm_nccl` package must be installed from source distribution\n# pip is too smart to store a wheel in the cache, and other CI jobs\n# will directly use the wheel from the cache, which is not what we want.\n# we need to remove it manually\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip cache remove vllm_nccl*\n#################### EXTENSION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.4.1-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd", "commit_date": "2024-05-13T14:57:07-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 setup.py bdist_wheel --dist-dir=dist\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n# the `vllm_nccl` package must be installed from source distribution\n# pip is too smart to store a wheel in the cache, and other CI jobs\n# will directly use the wheel from the cache, which is not what we want.\n# we need to remove it manually\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip cache remove vllm_nccl*\n#################### EXTENSION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.4.1-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "eb6d3c264d0cd8e44dec16bca7947fbe96415ce9", "commit_date": "2024-05-23T06:17:27+09:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 setup.py bdist_wheel --dist-dir=dist\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n# the `vllm_nccl` package must be installed from source distribution\n# pip is too smart to store a wheel in the cache, and other CI jobs\n# will directly use the wheel from the cache, which is not what we want.\n# we need to remove it manually\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip cache remove vllm_nccl*\n#################### EXTENSION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.4.1-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "a377f0bd5e1fa0ca069e3dbf28f4de5af64d0bb1", "commit_date": "2024-05-31T13:14:50+08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 setup.py bdist_wheel --dist-dir=dist\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.4.1-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "8d75fe48ca5f46b7af0f5201d8500b9604eed769", "commit_date": "2024-06-07T08:42:35+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 setup.py bdist_wheel --dist-dir=dist\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.4.1-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245", "commit_date": "2024-06-13T09:33:14-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git curl sudo\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=vllm-build-sccache \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.4.1-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "319ad7f1d386699e94f629341c9988a926821f24", "commit_date": "2024-06-13T22:36:20-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:12.4.1-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git curl sudo\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-cuda.txt\n\n# install development dependencies\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n\n#################### WHEEL BUILD IMAGE ####################\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=vllm-build-sccache \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:12.4.1-base-ubuntu22.04 AS vllm-base\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-12.4/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    pip install dist/*.whl --verbose\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer modelscope\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "7c01f706418d593b3cf23d2ec9110dca7151c539", "commit_date": "2024-06-29T12:47:53+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv python3-pip \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version \\\n    && python3 -m pip --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git curl sudo\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=vllm-build-sccache \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "3476ed0809ec91a3457da0cb90543133a4f4b519", "commit_date": "2024-07-01T20:10:37-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv python3-pip \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version \\\n    && python3 -m pip --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git curl sudo\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=vllm-build-sccache \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "2bb0489cb3367e46e201e84ab629df535544495b", "commit_date": "2024-07-16T08:13:25-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv python3-pip \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version \\\n    && python3 -m pip --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git curl sudo\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=vllm-build-sccache \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nWORKDIR /vllm-workspace\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.0.9/flashinfer-0.0.9+cu121torch2.3-cp310-cp310-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "9ed82e7074a18e25680ab106fc846364ad97bc00", "commit_date": "2024-07-19T12:10:56-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y git curl sudo\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3.10\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=vllm-build-sccache \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git curl\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.0.9/flashinfer-0.0.9+cu121torch2.3-cp310-cp310-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "9042d683620a7e3fa75c953fe9cca29086ce2b9a", "commit_date": "2024-07-20T04:17:24+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y git curl sudo\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3.10\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=vllm-build-sccache \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git curl libibverbs-dev\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.0.9/flashinfer-0.0.9+cu121torch2.3-cp310-cp310-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "89a84b0bb7b30706a02836234a94493ea8f780bf", "commit_date": "2024-07-25T21:31:31-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y git curl sudo\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3.10\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ \"$CUDA_VERSION\" = \"11.8.0\" ]; then \\\n            export SCCACHE_BUCKET=vllm-build-sccache-2; \\\n           else \\\n            export SCCACHE_BUCKET=vllm-build-sccache; \\\n           fi \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim curl libibverbs-dev\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.0.9/flashinfer-0.0.9+cu121torch2.3-cp310-cp310-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "a0dce9383ab7de0015060fb9fedadeb7d8ffdfb9", "commit_date": "2024-07-31T14:40:44-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y git curl sudo\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3.10\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n# make sure punica kernels are built (for LoRA)\nENV VLLM_INSTALL_PUNICA_KERNELS=1\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ \"$CUDA_VERSION\" = \"11.8.0\" ]; then \\\n            export SCCACHE_BUCKET=vllm-build-sccache-2; \\\n           else \\\n            export SCCACHE_BUCKET=vllm-build-sccache; \\\n           fi \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim curl libibverbs-dev\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.0.9/flashinfer-0.0.9+cu121torch2.3-cp310-cp310-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "6ce01f30667bbae33f112152e07a3b66b841078f", "commit_date": "2024-08-01T18:29:52-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y git curl sudo\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3.10\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ \"$CUDA_VERSION\" = \"11.8.0\" ]; then \\\n            export SCCACHE_BUCKET=vllm-build-sccache-2; \\\n           else \\\n            export SCCACHE_BUCKET=vllm-build-sccache; \\\n           fi \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim curl libibverbs-dev\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.2/flashinfer-0.1.2+cu121torch2.4-cp310-cp310-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce", "commit_date": "2024-08-06T12:34:25-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y git curl sudo\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3.10\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ \"$CUDA_VERSION\" = \"11.8.0\" ]; then \\\n            export SCCACHE_BUCKET=vllm-build-sccache-2; \\\n           else \\\n            export SCCACHE_BUCKET=vllm-build-sccache; \\\n           fi \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim curl libibverbs-dev\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.2/flashinfer-0.1.2+cu121torch2.4-cp310-cp310-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "e02ac5561748306186aaeaad6dad4c89484a2b45", "commit_date": "2024-08-08T21:34:28-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y git curl sudo\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3.10\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ \"$CUDA_VERSION\" = \"11.8.0\" ]; then \\\n            export SCCACHE_BUCKET=vllm-build-sccache-2; \\\n           else \\\n            export SCCACHE_BUCKET=vllm-build-sccache; \\\n           fi \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim curl libibverbs-dev\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.2/flashinfer-0.1.2+cu121torch2.4-cp310-cp310-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "fc7b8d1eefcbe837a56b7c080509417fe5167e6c", "commit_date": "2024-08-09T15:49:36+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y git curl sudo\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3.10\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ \"$CUDA_VERSION\" = \"11.8.0\" ]; then \\\n            export SCCACHE_BUCKET=vllm-build-sccache-2; \\\n           else \\\n            export SCCACHE_BUCKET=vllm-build-sccache; \\\n           fi \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim curl libibverbs-dev\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.2/flashinfer-0.1.2+cu121torch2.4-cp310-cp310-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "ab7165f2c7ea358df969d68a0fb0ce9bb184a083", "commit_date": "2024-08-18T01:15:10-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\n\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y git curl sudo\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\nARG PYTHON_VERSION=3.10\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# install compiler cache to speed up compilation leveraging local or remote caching\nRUN apt-get update -y && apt-get install -y ccache\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ \"$CUDA_VERSION\" = \"11.8.0\" ]; then \\\n            export SCCACHE_BUCKET=vllm-build-sccache-2; \\\n           else \\\n            export SCCACHE_BUCKET=vllm-build-sccache; \\\n           fi \\\n        && export SCCACHE_REGION=us-west-2 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\n\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && if [ \"${PYTHON_VERSION}\" != \"3\" ]; then update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1; fi \\\n    && python3 --version\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip git vim curl libibverbs-dev\n\n# Install pip s.t. it will be compatible with our PYTHON_VERSION\nRUN curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION}\nRUN python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.3/flashinfer-0.1.3+cu121torch2.4-cp310-cp310-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "2deb029d115dadd012ce5ea70487a207cb025493", "commit_date": "2024-08-26T11:24:53-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.4/flashinfer-0.1.4+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "e3580537a41a46b0f3cd750b86b633c1857a8c90", "commit_date": "2024-08-28T00:36:31-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nCOPY requirements-mamba.txt requirements-mamba.txt\nRUN python3 -m pip install packaging\nRUN python3 -m pip install -r requirements-mamba.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### MAMBA Build IMAGE ####################\nFROM dev as mamba-builder\n# max jobs used for build\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n\nWORKDIR /usr/src/mamba\n\nCOPY requirements-mamba.txt requirements-mamba.txt\n\n# Download the wheel or build it if a pre-compiled release doesn't exist\nRUN pip --verbose wheel -r requirements-mamba.txt \\\n    --no-build-isolation --no-deps --no-cache-dir\n\n#################### MAMBA Build IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=bind,from=mamba-builder,src=/usr/src/mamba,target=/usr/src/mamba \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install /usr/src/mamba/*.whl --no-cache-dir\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.4/flashinfer-0.1.4+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440", "commit_date": "2024-08-28T16:10:12-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.4/flashinfer-0.1.4+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "6e36f4fa6ce64619b9ea94c88a157f5783a63a65", "commit_date": "2024-09-02T14:20:12-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "bd852f2a8b9e9129de69fa7349906a9115538d5a", "commit_date": "2024-09-03T10:49:18-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e", "commit_date": "2024-09-03T18:50:29+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# files and directories related to build wheels\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY cmake cmake\nCOPY CMakeLists.txt CMakeLists.txt\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-adag.txt requirements-adag.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm vllm\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG buildkite_commit\nENV BUILDKITE_COMMIT=${buildkite_commit}\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# check the size of the wheel, we cannot upload wheels larger than 100MB\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\nRUN python3 check-wheel-size.py dist\n\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu20.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.10\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "83450458339b07765b0e72a822e5fe93eeaf5258", "commit_date": "2024-10-16T13:37:45-06:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# files and directories related to build wheels\nCOPY . .\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0' bitsandbytes>=0.44.0 timm==0.9.10\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "81ede99ca44a5b3518932a07ea4a76a719e7416e", "commit_date": "2024-10-17T11:38:15-05:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n# files and directories related to build wheels\nCOPY . .\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0' bitsandbytes>=0.44.0 timm==0.9.10\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "b2e0ad3b598ed0e022cdbd678a20821d411873c2", "commit_date": "2024-11-15T00:38:20+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.44.0' timm==0.9.10\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f", "commit_date": "2024-11-28T08:31:28-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.44.0' timm==0.9.10\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "98f47f2a4032f8c395268de80858c64ffcfc60fa", "commit_date": "2024-11-28T09:01:02-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.44.0' timm==0.9.10\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "9323a3153b20d4a2ca7ac04a2784609d6ce656e0", "commit_date": "2024-12-03T15:17:00+08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.44.0' timm==0.9.10\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "3b61cb450d899dc423feb264c297d4d18d701678", "commit_date": "2024-12-09T12:38:46-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.44.0' timm==0.9.10\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "f092153fbe349a9a1742940e3703bfcff6aa0a6d", "commit_date": "2024-12-11T23:14:20-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.44.0' timm==0.9.10\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "886936837ca89e5645bc1f71cc0e1492b65b1590", "commit_date": "2024-12-14T11:38:10-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.44.0' timm==0.9.10\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "25ebed2f8ca6d747d63f2be9ede023c561851ac8", "commit_date": "2024-12-15T13:33:00-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n#################### DEV IMAGE ####################\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# install vllm wheel first, so that torch etc will be installed\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    . /etc/environment && \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' timm==0.9.10\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "f26c4aeecba481ce1445be7a998b0b97460a13bb", "commit_date": "2024-12-18T23:38:02-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.rst and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nCOPY requirements-cuda-arm64.txt requirements-cuda-arm64.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install -r requirements-cuda-arm64.txt; \\\n    fi\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install -r requirements-cuda-arm64.txt; \\\n    fi\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nCOPY requirements-cuda-arm64.txt requirements-cuda-arm64.txt\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip uninstall -y torch && \\\n        python3 -m pip install -r requirements-cuda-arm64.txt; \\\n    fi\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10'; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10'; \\\n    fi\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "b55ed6ef8ab0dce7fb0f79ff292dafdb4d22610c", "commit_date": "2025-01-02T12:04:58-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/dev/dockerfile/dockerfile.md and\n# docs/source/assets/dev/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "526de822d501c792b051c864ba873a836d78d5bf", "commit_date": "2025-01-08T20:23:15+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "310aca88c984983189a57f1b72e3b1dde89fb92f", "commit_date": "2025-01-09T07:18:21+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "3127e975fb9417d10513e25b80820870f594c627", "commit_date": "2025-01-20T17:36:24+08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "aea94362c9bdd08ed2b346701bdc09d278e85f66", "commit_date": "2025-01-22T22:22:12+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "6dd94dbe94c1820a1e224cba65efcf0befa97995", "commit_date": "2025-01-24T11:34:27+08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412", "commit_date": "2025-01-26T00:42:37-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=300\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8", "commit_date": "2025-01-31T15:37:30-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=300\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\n# How to build this FlashInfer wheel:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.0.post1-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "baeded25699f9f4851843306f27f685c4d4ee7c5", "commit_date": "2025-01-31T21:52:51-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=300\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\n# How to build this FlashInfer wheel:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.0.post1-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "b9986454fe8ba80e2a109d069397b6b59aae658b", "commit_date": "2025-02-03T13:46:19+08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=300\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\n# How to build this FlashInfer wheel:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.0.post1-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "5e5c8e091eacc16672a0a8265eb5cb0ece85d24b", "commit_date": "2025-02-14T12:53:42-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\n# How to build this FlashInfer wheel:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.0.post1-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "30172b4947c52890b808c6da3a6c7580f55cbb74", "commit_date": "2025-02-18T12:15:33-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "4c822298981a8f7521492075ff72659985fc4c3f", "commit_date": "2025-02-18T13:19:58-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "0d243f2a54fbd1c56da8a571f0899c30b6aba5d9", "commit_date": "2025-02-20T04:01:02+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "6a417b8600d4d1e57698a91b71a38446e8fc5c45", "commit_date": "2025-02-20T10:59:36-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install uv\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    uv pip install --system -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    uv pip install --system -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    uv pip install --system -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install uv\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    uv pip install --system -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    uv pip install --system -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "e206b5433109d298e53451015465b2bf8f03ef0a", "commit_date": "2025-02-26T14:58:24+08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "19d98e0c7db96713f0e2201649159431177a56e2", "commit_date": "2025-03-03T16:29:53-05:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "9badee53decb3d432dc805336abfb0eb81dfb48f", "commit_date": "2025-03-04T20:59:22+01:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "9f1710f1ace3535920c0bb6d4cc329c36289080e", "commit_date": "2025-03-06T09:35:49-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "dae68969774e41b93b01cd31171ca033a92b574a", "commit_date": "2025-03-06T19:59:14-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install minimal dependencies and uv\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install minimal dependencies and uv\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo vim \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 libibverbs-dev \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "1e3598edeb694c1a6a1fdfac5e7d12697749561b", "commit_date": "2025-03-07T13:25:13+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install minimal dependencies and uv\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install minimal dependencies and uv\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo vim \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 libibverbs-dev \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "ca7a2d5f28eac9621474563cdda0e08596222755", "commit_date": "2025-03-07T22:18:53-08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install minimal dependencies and uv\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install minimal dependencies and uv\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo vim \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 libibverbs-dev \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "fb0acb6c72874e98617cabee4ff4851569374fc9", "commit_date": "2025-03-10T12:06:58-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo vim \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 libibverbs-dev \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "70b808fe1a63322bc6bf5f46a91981a8f6b8af00", "commit_date": "2025-03-11T07:39:56+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo vim \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 libibverbs-dev \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "fe66b34728e5d383e3d19aefc544eeee808c99fb", "commit_date": "2025-03-14T16:36:18-04:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo vim \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 libibverbs-dev \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post1/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "ccf02fcbaebb1a5b59dfc6c7cb64aa7cc489f04c", "commit_date": "2025-03-14T20:45:42-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo vim \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 libibverbs-dev \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "99abb8b650c66664cdc84d815b7f306f33bd9881", "commit_date": "2025-03-18T14:31:54-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo vim \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 libibverbs-dev \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "22d33baca2c0c639cfd45c48e99803e56c3efa74", "commit_date": "2025-03-19T21:04:41+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo vim \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 libibverbs-dev \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "296f927f2493908984707354e3cc5d7b2e41650b", "commit_date": "2025-03-20T19:21:08-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu126 \"torch==2.7.0.dev20250121+cu126\" \"torchvision==0.22.0.dev20250121\";  \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install minimal dependencies and uv\nRUN apt-get update -y \\\n    && apt-get install -y ccache git curl wget sudo vim \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 libibverbs-dev \\\n    && curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Add uv to PATH\nENV PATH=\"/root/.local/bin:$PATH\"\n# Create venv with specified Python and activate by placing at the front of path\nENV VIRTUAL_ENV=\"/opt/venv\"\nRUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "9d72daf4ced05a5fec1ad8ea2914a39296f402da", "commit_date": "2025-03-24T22:44:08+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "35fad35a485eac9195c510731ba4a9d297dfd963", "commit_date": "2025-03-26T10:56:47-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "b10e51989551cd80dd74079429ccf91f0807bd92", "commit_date": "2025-04-06T20:48:14+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "bd6028d6b0bbc0c569ece0535067081c5e8bdc14", "commit_date": "2025-04-12T14:21:08+08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "93e5f3c5fb4a4bbd49610efb96aad30df95fca66", "commit_date": "2025-04-12T22:54:37+08:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "3092375e274e9e003961e600e10a6192d33ceaa0", "commit_date": "2025-04-16T19:28:32-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "05fcd1b4308aa2dca9a1b24c540b57a94a7ba124", "commit_date": "2025-04-17T07:45:24-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "299ebb62b269ce167eb1c71b5e39a1dc1f65ce1c", "commit_date": "2025-04-21T18:18:22+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "e4d614423283d8beae4a92aeb537a49ad2662864", "commit_date": "2025-04-22T08:16:19+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "bc7c4d206bbfb56b06d218b6c2971e8ca191db36", "commit_date": "2025-04-22T19:11:56-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "41ca7eb49192cbee65986527319c6eb929d6aa7c", "commit_date": "2025-04-24T20:12:21-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system mamba-ssm==2.2.4 --no-build-isolation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.1.post2/flashinfer_python-0.2.1.post2+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install development dependencies (for testing)\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system mamba-ssm==2.2.4 --no-build-isolation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "015069b01741e9ecb9e604c7fe87fbdfc306ebe5", "commit_date": "2025-05-01T03:29:01-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # TESTING: install FlashInfer from source to test 2.7.0 final RC\n    FLASHINFER_ENABLE_AOT=1 TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX' \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/flashinfer-ai/flashinfer@v0.2.2.post1\" ; \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "022afbeb4efa22bb8a4656a2712cd66c6a811c23", "commit_date": "2025-05-07T00:36:41+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # TESTING: install FlashInfer from source to test 2.7.0 final RC\n    FLASHINFER_ENABLE_AOT=1 TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX' \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/flashinfer-ai/flashinfer@v0.2.2.post1\" ; \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "67da5720d4ed2aa1f615ec812031f4f3753b3f62", "commit_date": "2025-05-15T23:31:02-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # TESTING: install FlashInfer from source to test 2.7.0 final RC\n    FLASHINFER_ENABLE_AOT=1 TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX' \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/flashinfer-ai/flashinfer@v0.2.2.post1\" ; \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Although we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "e493e48524e9e78ab33eafec6461b3940e361189", "commit_date": "2025-05-23T03:38:23-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # uv pip install --system https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.4/flashinfer_python-0.2.4+cu124torch2.6-cp38-abi3-linux_x86_64.whl ; \\\n    # TESTING: install FlashInfer from source to test 2.7.0 final RC\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0 10.0+PTX'; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0+PTX'; \\\n    fi; \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -lt 12 ]; then \\\n        export FLASHINFER_ENABLE_SM90=0; \\\n    fi; \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/flashinfer-ai/flashinfer@21ea1d2545f74782b91eb8c08fd503ac4c0743fc\" ; \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "d55e446d1320d0f5f22bc3584f81f18d7924f166", "commit_date": "2025-05-24T06:51:22+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer alreary has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.5%2Bcu128torch2.7-cp38-abi3-linux_x86_64.whl; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0+PTX'; \\\n        CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n        if [ \"$CUDA_MAJOR\" -lt 12 ]; then \\\n            export FLASHINFER_ENABLE_SM90=0; \\\n        fi; \\\n        uv pip install --system --no-build-isolation \"git+https://github.com/flashinfer-ai/flashinfer@21ea1d2545f74782b91eb8c08fd503ac4c0743fc\" ; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "e7523c2e031bc96740723ab63833d1cf94229ab4", "commit_date": "2025-05-26T11:49:36-04:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer alreary has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.5%2Bcu128torch2.7-cp38-abi3-linux_x86_64.whl; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0+PTX'; \\\n        CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n        if [ \"$CUDA_MAJOR\" -lt 12 ]; then \\\n            export FLASHINFER_ENABLE_SM90=0; \\\n        fi; \\\n        uv pip install --system --no-build-isolation \"git+https://github.com/flashinfer-ai/flashinfer@21ea1d2545f74782b91eb8c08fd503ac4c0743fc\" ; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "7661e92ef85e552936195ae4b803e292b9a96776", "commit_date": "2025-06-06T10:05:14+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ export FLASHINFER_ENABLE_AOT=1\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.6 8.9 9.0+PTX'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout 524304395bd1d8cd7d07db083859523fcaa246a4\n# $ rm -rf build\n# $ python3 setup.py bdist_wheel --dist-dir=dist --verbose\n# $ ls dist\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/524304395bd1d8cd7d07db083859523fcaa246a4/flashinfer_python-0.2.1.post1+cu124torch2.5-cp38-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer alreary has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.5%2Bcu128torch2.7-cp38-abi3-linux_x86_64.whl; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0+PTX'; \\\n        CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n        if [ \"$CUDA_MAJOR\" -lt 12 ]; then \\\n            export FLASHINFER_ENABLE_SM90=0; \\\n        fi; \\\n        uv pip install --system --no-build-isolation \"git+https://github.com/flashinfer-ai/flashinfer@21ea1d2545f74782b91eb8c08fd503ac4c0743fc\" ; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "2c5302fadd81c06f61e5a3973ed4c0e6a4a2be40", "commit_date": "2025-06-21T20:01:07+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a' && \\\n        git clone https://github.com/flashinfer-ai/flashinfer.git --single-branch --branch v0.2.6.post1 --recursive && \\\n        # Needed to build AOT kernels\n        (cd flashinfer && \\\n            python3 -m flashinfer.aot && \\\n            uv pip install --system --no-build-isolation . \\\n        ) && \\\n        rm -rf flashinfer; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "c4cf26067755624eb94afda36ae1f679dec0d542", "commit_date": "2025-06-22T23:11:22+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a' && \\\n        git clone https://github.com/flashinfer-ai/flashinfer.git --single-branch --branch v0.2.6.post1 --recursive && \\\n        # Needed to build AOT kernels\n        (cd flashinfer && \\\n            python3 -m flashinfer.aot && \\\n            uv pip install --system --no-build-isolation . \\\n        ) && \\\n        rm -rf flashinfer; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "9a3b88328f7e434cac35b90ee463de6689f9a833", "commit_date": "2025-06-23T23:01:26-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a' && \\\n        git clone https://github.com/flashinfer-ai/flashinfer.git --single-branch --branch v0.2.6.post1 --recursive && \\\n        # Needed to build AOT kernels\n        (cd flashinfer && \\\n            python3 -m flashinfer.aot && \\\n            uv pip install --system --no-build-isolation . \\\n        ) && \\\n        rm -rf flashinfer; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "a045b7e89a2424ec3b152ee57be2931eedb2abd2", "commit_date": "2025-06-24T13:09:01-04:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a' && \\\n        git clone https://github.com/flashinfer-ai/flashinfer.git --single-branch --branch v0.2.6.post1 --recursive && \\\n        # Needed to build AOT kernels\n        (cd flashinfer && \\\n            python3 -m flashinfer.aot && \\\n            uv pip install --system --no-build-isolation . \\\n        ) && \\\n        rm -rf flashinfer; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "3443aaf8dd7cb0e2f40dd1e6d9d36c8db23c6597", "commit_date": "2025-06-24T20:33:51-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a' && \\\n        git clone https://github.com/flashinfer-ai/flashinfer.git --single-branch --branch v0.2.6.post1 --recursive && \\\n        # Needed to build AOT kernels\n        (cd flashinfer && \\\n            python3 -m flashinfer.aot && \\\n            uv pip install --system --no-build-isolation . \\\n        ) && \\\n        rm -rf flashinfer; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "7108934142801dad2fd8ac42aec8b1699e37ff5d", "commit_date": "2025-06-25T00:41:11-04:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a' && \\\n        git clone https://github.com/flashinfer-ai/flashinfer.git --single-branch --branch v0.2.6.post1 --recursive && \\\n        # Needed to build AOT kernels\n        (cd flashinfer && \\\n            python3 -m flashinfer.aot && \\\n            uv pip install --system --no-build-isolation . \\\n        ) && \\\n        rm -rf flashinfer; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "6e244ae09121b2c1cdcd4db51076decc4a724c5c", "commit_date": "2025-06-27T00:44:14-04:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0' && \\\n        git clone https://github.com/flashinfer-ai/flashinfer.git --single-branch --branch v0.2.6.post1 --recursive && \\\n        # Needed to build AOT kernels\n        (cd flashinfer && \\\n            python3 -m flashinfer.aot && \\\n            uv pip install --system --no-build-isolation . \\\n        ) && \\\n        rm -rf flashinfer; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "aa0dc77ef53b365ddf54be51748c166895a0bcd9", "commit_date": "2025-06-27T09:16:41+00:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && for i in 1 2 3; do \\\n        add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n        { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n    done \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system --index-url https://download.pytorch.org/whl/nightly/cu128 --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0' && \\\n        git clone https://github.com/flashinfer-ai/flashinfer.git --single-branch --branch v0.2.6.post1 --recursive && \\\n        # Needed to build AOT kernels\n        (cd flashinfer && \\\n            python3 -m flashinfer.aot && \\\n            uv pip install --system --no-build-isolation . \\\n        ) && \\\n        rm -rf flashinfer; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\   \n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "c05596f1a350f3d993c467959ed02492141c2527", "commit_date": "2025-07-01T05:10:28-04:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Allow specifying a version, Git revision or local .whl file\nARG FLASHINFER_CUDA128_INDEX_URL=\"https://download.pytorch.org/whl/cu128/flashinfer\"\nARG FLASHINFER_CUDA128_WHEEL=\"flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl\"\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.6.post1\"\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n    if [[ \"$CUDA_VERSION\" == 12.8* ]]; then \\\n        uv pip install --system ${FLASHINFER_CUDA128_INDEX_URL}/${FLASHINFER_CUDA128_WHEEL} ; \\\n    else \\\n        export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0' && \\\n        git clone ${FLASHINFER_GIT_REPO} --single-branch --branch ${FLASHINFER_GIT_REF} --recursive && \\\n        # Needed to build AOT kernels\n        (cd flashinfer && \\\n            python3 -m flashinfer.aot && \\\n            uv pip install --system --no-build-isolation . \\\n        ) && \\\n        rm -rf flashinfer; \\\n    fi \\\nfi\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "9965c47d0d072a74252878ec3c9963a540e9a690", "commit_date": "2025-07-02T17:50:25-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Allow specifying a version, Git revision or local .whl file\nARG FLASHINFER_CUDA128_INDEX_URL=\"https://download.pytorch.org/whl/cu128/flashinfer\"\nARG FLASHINFER_CUDA128_WHEEL=\"flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl\"\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.6.post1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n  if [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then\n      # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n      if [[ \"$CUDA_VERSION\" == 12.8* ]]; then\n          uv pip install --system ${FLASHINFER_CUDA128_INDEX_URL}/${FLASHINFER_CUDA128_WHEEL}\n      else\n          export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n          git clone ${FLASHINFER_GIT_REPO} --single-branch --branch ${FLASHINFER_GIT_REF} --recursive\n          # Needed to build AOT kernels\n          (cd flashinfer && \\\n              python3 -m flashinfer.aot && \\\n              uv pip install --system --no-build-isolation . \\\n          )\n          rm -rf flashinfer\n\n          # Default arches (skipping 10.0a and 12.0 since these need 12.8)\n          # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n          TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n          if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n              TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n          fi\n          echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${TORCH_CUDA_ARCH_LIST}\"\n\n          git clone --depth 1 --recursive --shallow-submodules \\\n            --branch v0.2.6.post1 \\\n            https://github.com/flashinfer-ai/flashinfer.git flashinfer\n\n          pushd flashinfer\n            python3 -m flashinfer.aot\n            TORCH_CUDA_ARCH_LIST=\"${TORCH_CUDA_ARCH_LIST}\" \\\n              uv pip install --system --no-build-isolation .\n          popd\n\n          rm -rf flashinfer\n      fi \\\n  fi\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.3' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "22dd9c2730dc1124b9d0ac15fff223d0b8d9020b", "commit_date": "2025-07-07T19:08:12+00:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables build-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Allow specifying a version, Git revision or local .whl file\nARG FLASHINFER_CUDA128_INDEX_URL=\"https://download.pytorch.org/whl/cu128/flashinfer\"\nARG FLASHINFER_CUDA128_WHEEL=\"flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl\"\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.6.post1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n  if [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then\n      # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n      if [[ \"$CUDA_VERSION\" == 12.8* ]]; then\n          uv pip install --system ${FLASHINFER_CUDA128_INDEX_URL}/${FLASHINFER_CUDA128_WHEEL}\n      else\n          export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n          git clone ${FLASHINFER_GIT_REPO} --single-branch --branch ${FLASHINFER_GIT_REF} --recursive\n          # Needed to build AOT kernels\n          (cd flashinfer && \\\n              python3 -m flashinfer.aot && \\\n              uv pip install --system --no-build-isolation . \\\n          )\n          rm -rf flashinfer\n\n          # Default arches (skipping 10.0a and 12.0 since these need 12.8)\n          # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n          TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n          if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n              TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n          fi\n          echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${TORCH_CUDA_ARCH_LIST}\"\n\n          git clone --depth 1 --recursive --shallow-submodules \\\n            --branch v0.2.6.post1 \\\n            https://github.com/flashinfer-ai/flashinfer.git flashinfer\n\n          pushd flashinfer\n            python3 -m flashinfer.aot\n            TORCH_CUDA_ARCH_LIST=\"${TORCH_CUDA_ARCH_LIST}\" \\\n              uv pip install --system --no-build-isolation .\n          popd\n\n          rm -rf flashinfer\n      fi \\\n  fi\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.46.1' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "31c5d0a1b79b0b4621d71a069990d8af43976b08", "commit_date": "2025-07-07T19:04:54-07:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables build-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Allow specifying a version, Git revision or local .whl file\nARG FLASHINFER_CUDA128_INDEX_URL=\"https://download.pytorch.org/whl/cu128/flashinfer\"\nARG FLASHINFER_CUDA128_WHEEL=\"flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl\"\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.6.post1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n  if [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then\n      # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n      if [[ \"$CUDA_VERSION\" == 12.8* ]]; then\n          uv pip install --system ${FLASHINFER_CUDA128_INDEX_URL}/${FLASHINFER_CUDA128_WHEEL}\n      else\n          export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n          git clone ${FLASHINFER_GIT_REPO} --single-branch --branch ${FLASHINFER_GIT_REF} --recursive\n          # Needed to build AOT kernels\n          (cd flashinfer && \\\n              python3 -m flashinfer.aot && \\\n              uv pip install --system --no-build-isolation . \\\n          )\n          rm -rf flashinfer\n\n          # Default arches (skipping 10.0a and 12.0 since these need 12.8)\n          # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n          TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n          if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n              TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n          fi\n          echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${TORCH_CUDA_ARCH_LIST}\"\n\n          git clone --depth 1 --recursive --shallow-submodules \\\n            --branch v0.2.6.post1 \\\n            https://github.com/flashinfer-ai/flashinfer.git flashinfer\n\n          pushd flashinfer\n            python3 -m flashinfer.aot\n            TORCH_CUDA_ARCH_LIST=\"${TORCH_CUDA_ARCH_LIST}\" \\\n              uv pip install --system --no-build-isolation .\n          popd\n\n          rm -rf flashinfer\n      fi \\\n  fi\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.46.1' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "c0569dbc82b5e945a77878190114d1b68027828b", "commit_date": "2025-07-14T19:47:16+00:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables build-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Allow specifying a version, Git revision or local .whl file\nARG FLASHINFER_CUDA128_INDEX_URL=\"https://download.pytorch.org/whl/cu128/flashinfer\"\nARG FLASHINFER_CUDA128_WHEEL=\"flashinfer_python-0.2.6.post1%2Bcu128torch2.7-cp39-abi3-linux_x86_64.whl\"\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.8rc1\"\n# Flag to control whether to use pre-built FlashInfer wheels (set to false to force build from source)\n# TODO: Currently disabled because the pre-built wheels are not available for FLASHINFER_GIT_REF\nARG USE_FLASHINFER_PREBUILT_WHEEL=false\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n  if [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then\n      # FlashInfer already has a wheel for PyTorch 2.7.0 and CUDA 12.8. This is enough for CI use\n      if [[ \"$CUDA_VERSION\" == 12.8* ]] && [[ \"$USE_FLASHINFER_PREBUILT_WHEEL\" == \"true\" ]]; then\n          uv pip install --system ${FLASHINFER_CUDA128_INDEX_URL}/${FLASHINFER_CUDA128_WHEEL}\n      else\n          # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n          # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n          if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n              FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n          elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n              FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n          else\n              # CUDA 12.8+ supports 10.0a and 12.0\n              FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n          fi\n          echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n\n          git clone --depth 1 --recursive --shallow-submodules \\\n            --branch ${FLASHINFER_GIT_REF} \\\n            ${FLASHINFER_GIT_REPO} flashinfer\n\n          # Needed to build AOT kernels\n          pushd flashinfer\n            TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n              python3 -m flashinfer.aot\n            TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n              uv pip install --system --no-build-isolation .\n          popd\n\n          rm -rf flashinfer\n      fi \\\n  fi\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.46.1' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "8a4e5c5f3c1d39e924e48a87c9cc6cf382aa3532", "commit_date": "2025-07-16T22:13:00-07:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.8rc1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "dcc6cfb991cd76369aad96e04424f29c8fecdbd8", "commit_date": "2025-07-18T23:09:51-07:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.8rc1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "6d0734c562e759fdb7076d762222b3881e62ab1f", "commit_date": "2025-07-19T02:33:01-07:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.8rc1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer 'modelscope!=1.15.0' \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add", "commit_date": "2025-07-21T11:19:23-07:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.8rc1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "e7b204268132cb775c139574c1ff4ad7e15c8f66", "commit_date": "2025-07-21T21:49:01-07:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.8rc1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "a32237665df876fcb51196dc209e8aff9fd89d29", "commit_date": "2025-07-22T05:27:18-07:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.8rc1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "ed25054577f7abca2aee32a5290200c4a1aed561", "commit_date": "2025-07-22T06:17:47-07:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.8rc1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9", "commit_date": "2025-07-22T07:07:44-07:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.8rc1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "61b8cea3b42feab021d506e9143551de18f9165c", "commit_date": "2025-07-24T03:21:46-07:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base AS dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.8\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for #17068\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system --no-build-isolation \"git+https://github.com/state-spaces/mamba@v2.2.4\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "ec261b0291568750f86ccc74016e92e995f70f0f", "commit_date": "2025-07-28T16:43:37+00:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base AS dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.9rc1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8", "commit_date": "2025-07-28T18:49:04-04:00", "Dockerfile": "\n# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base AS dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\nARG FLASHINFER_GIT_REF=\"v0.2.9rc2\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "ac45c44d98e77f30e47b8fb69134f4635183070d", "commit_date": "2025-08-01T10:14:38-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base AS dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\n# Keep this in sync with https://github.com/vllm-project/vllm/blob/main/requirements/cuda.txt\n# We use `--force-reinstall --no-deps` to avoid issues with the existing FlashInfer wheel.\nARG FLASHINFER_GIT_REF=\"v0.2.9rc2\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation --force-reinstall --no-deps .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# Install DeepGEMM from source\nARG DEEPGEMM_GIT_REPO=\"https://github.com/deepseek-ai/DeepGEMM.git\"\nARG DEEPGEMM_GIT_REF=\"187656694f7f69e3e7975617a68bc3387680a7e1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"\n    CUDA_MINOR=\"${CUDA_VERSION#${CUDA_MAJOR}.}\"\n    CUDA_MINOR=\"${CUDA_MINOR%%.*}\"\n    if [ \"$CUDA_MAJOR\" -ge 12 ] && [ \"$CUDA_MINOR\" -ge 8 ]; then\n        git clone --recursive --shallow-submodules \\\n            ${DEEPGEMM_GIT_REPO} deepgemm\n        echo \"\ud83c\udfd7\ufe0f  Building DeepGEMM\"\n        pushd deepgemm\n            git checkout ${DEEPGEMM_GIT_REF}\n            # Build DeepGEMM\n            # (Based on https://github.com/deepseek-ai/DeepGEMM/blob/main/install.sh)\n            rm -rf build dist\n            rm -rf *.egg-info\n            python3 setup.py bdist_wheel\n            uv pip install --system dist/*.whl\n        popd\n        rm -rf deepgemm\n    else\n        echo \"Skipping DeepGEMM installation (requires CUDA 12.8+ but got ${CUDA_VERSION})\"\n    fi\nBASH\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "eefbf4a68b7b0a5b8364a59647906be1b7f043e2", "commit_date": "2025-08-01T19:18:51-04:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base AS dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\n# Keep this in sync with https://github.com/vllm-project/vllm/blob/main/requirements/cuda.txt\n# We use `--force-reinstall --no-deps` to avoid issues with the existing FlashInfer wheel.\nARG FLASHINFER_GIT_REF=\"v0.2.9rc2\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation --force-reinstall --no-deps .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# Install DeepGEMM from source\nARG DEEPGEMM_GIT_REPO=\"https://github.com/deepseek-ai/DeepGEMM.git\"\nARG DEEPGEMM_GIT_REF=\"187656694f7f69e3e7975617a68bc3387680a7e1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"\n    CUDA_MINOR=\"${CUDA_VERSION#${CUDA_MAJOR}.}\"\n    CUDA_MINOR=\"${CUDA_MINOR%%.*}\"\n    if [ \"$CUDA_MAJOR\" -ge 12 ] && [ \"$CUDA_MINOR\" -ge 8 ]; then\n        git clone --recursive --shallow-submodules \\\n            ${DEEPGEMM_GIT_REPO} deepgemm\n        echo \"\ud83c\udfd7\ufe0f  Building DeepGEMM\"\n        pushd deepgemm\n            git checkout ${DEEPGEMM_GIT_REF}\n            # Build DeepGEMM\n            # (Based on https://github.com/deepseek-ai/DeepGEMM/blob/main/install.sh)\n            rm -rf build dist\n            rm -rf *.egg-info\n            python3 setup.py bdist_wheel\n            uv pip install --system dist/*.whl\n        popd\n        rm -rf deepgemm\n    else\n        echo \"Skipping DeepGEMM installation (requires CUDA 12.8+ but got ${CUDA_VERSION})\"\n    fi\nBASH\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "58eee5f2e05b74eb2cb1a3bbda9c04df4805e4cc", "commit_date": "2025-08-02T01:43:52-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base AS dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\n# Keep this in sync with https://github.com/vllm-project/vllm/blob/main/requirements/cuda.txt\n# We use `--force-reinstall --no-deps` to avoid issues with the existing FlashInfer wheel.\nARG FLASHINFER_GIT_REF=\"v0.2.9rc2\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation --force-reinstall --no-deps .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# Install DeepGEMM from source\nARG DEEPGEMM_GIT_REPO=\"https://github.com/deepseek-ai/DeepGEMM.git\"\nARG DEEPGEMM_GIT_REF=\"187656694f7f69e3e7975617a68bc3387680a7e1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"\n    CUDA_MINOR=\"${CUDA_VERSION#${CUDA_MAJOR}.}\"\n    CUDA_MINOR=\"${CUDA_MINOR%%.*}\"\n    if [ \"$CUDA_MAJOR\" -ge 12 ] && [ \"$CUDA_MINOR\" -ge 8 ]; then\n        git clone --recursive --shallow-submodules \\\n            ${DEEPGEMM_GIT_REPO} deepgemm\n        echo \"\ud83c\udfd7\ufe0f  Building DeepGEMM\"\n        pushd deepgemm\n            git checkout ${DEEPGEMM_GIT_REF}\n            # Build DeepGEMM\n            # (Based on https://github.com/deepseek-ai/DeepGEMM/blob/main/install.sh)\n            rm -rf build dist\n            rm -rf *.egg-info\n            python3 setup.py bdist_wheel\n            uv pip install --system dist/*.whl\n        popd\n        rm -rf deepgemm\n    else\n        echo \"Skipping DeepGEMM installation (requires CUDA 12.8+ but got ${CUDA_VERSION})\"\n    fi\nBASH\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
{"commit": "b690e34824fd5a5c4054a0c0468ebfb6aa1dd215", "commit_date": "2025-08-02T01:59:34-07:00", "Dockerfile": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/contributing/dockerfile/dockerfile.md and\n# docs/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.8.1\nARG PYTHON_VERSION=3.12\n\n# By parameterizing the base images, we allow third-party to use their own\n# base images. One use case is hermetic builds with base images stored in\n# private registries that use a different repository naming conventions.\n#\n# Example:\n# docker build --build-arg BUILD_BASE_IMAGE=registry.acme.org/mirror/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\nARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04\n# TODO: Restore to base image after FlashInfer AOT wheel fixed\nARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04\n\n# By parameterizing the Deadsnakes repository URL, we allow third-party to use\n# their own mirror. When doing so, we don't benefit from the transparent\n# installation of the GPG key of the PPA, as done by add-apt-repository, so we\n# also need a URL for the GPG key.\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\n\n# The PyPA get-pip.py script is a self contained script+zip file, that provides\n# both the installer script and the pip base85-encoded zip archive. This allows\n# bootstrapping pip in environment where a dsitribution package does not exist.\n#\n# By parameterizing the URL for get-pip.py installation script, we allow\n# third-party to use their own copy of the script stored in a private mirror.\n# We set the default value to the PyPA owned get-pip.py script.\n#\n# Reference: https://pip.pypa.io/en/stable/installation/#get-pip-py\nARG GET_PIP_URL=\"https://bootstrap.pypa.io/get-pip.py\"\n\n# PIP supports fetching the packages from custom indexes, allowing third-party\n# to host the packages in private mirrors. The PIP_INDEX_URL and\n# PIP_EXTRA_INDEX_URL are standard PIP environment variables to override the\n# default indexes. By letting them empty by default, PIP will use its default\n# indexes if the build process doesn't override the indexes.\n#\n# Uv uses different variables. We set them by default to the same values as\n# PIP, but they can be overridden.\nARG PIP_INDEX_URL\nARG PIP_EXTRA_INDEX_URL\nARG UV_INDEX_URL=${PIP_INDEX_URL}\nARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\n\n# PyTorch provides its own indexes for standard and nightly builds\nARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly\n\n# PIP supports multiple authentication schemes, including keyring\n# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to\n# disabled by default, we allow third-party to use keyring authentication for\n# their private Python indexes, while not changing the default behavior which\n# is no authentication.\n#\n# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support\nARG PIP_KEYRING_PROVIDER=disabled\nARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}\n\n# Flag enables built-in KV-connector dependency libs into docker images\nARG INSTALL_KV_CONNECTORS=false\n\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM ${BUILD_BASE_IMAGE} AS base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\nENV DEBIAN_FRONTEND=noninteractive\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\";  \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40; \\\n    fi\n\nCOPY requirements/common.txt requirements/common.txt\nCOPY requirements/cuda.txt requirements/cuda.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/cuda.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# install build dependencies\nCOPY requirements/build.txt requirements/build.txt\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != \"0\" ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_DOWNLOAD_URL=https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz\nARG SCCACHE_ENDPOINT\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n\n# Flag to control whether to use pre-built vLLM wheels\nARG VLLM_USE_PRECOMPILED\n# TODO: in setup.py VLLM_USE_PRECOMPILED is sensitive to truthiness, it will take =0 as \"true\", this should be fixed\nENV VLLM_USE_PRECOMPILED=\"\"\nRUN if [ \"${VLLM_USE_PRECOMPILED}\" = \"1\" ]; then \\\n        export VLLM_USE_PRECOMPILED=1 && \\\n        echo \"Using precompiled wheels\"; \\\n    else \\\n        unset VLLM_USE_PRECOMPILED && \\\n        echo \"Leaving VLLM_USE_PRECOMPILED unset to build wheels from source\"; \\\n    fi\n\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz ${SCCACHE_DOWNLOAD_URL} \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && if [ ! -z ${SCCACHE_ENDPOINT} ] ; then export SCCACHE_ENDPOINT=${SCCACHE_ENDPOINT} ; fi \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        # Clean any existing CMake artifacts\n        rm -rf .deps && \\\n        mkdir -p .deps && \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# sync the default value with .buildkite/check-wheel-size.py\nARG VLLM_MAX_SIZE_MB=400\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base AS dev\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\nCOPY requirements/lint.txt requirements/lint.txt\nCOPY requirements/test.txt requirements/test.txt\nCOPY requirements/dev.txt requirements/dev.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/dev.txt \\\n    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM ${FINAL_BASE_IMAGE} AS vllm-base\nARG CUDA_VERSION\nARG PYTHON_VERSION\nARG INSTALL_KV_CONNECTORS=false\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nSHELL [\"/bin/bash\", \"-c\"]\n\nARG DEADSNAKES_MIRROR_URL\nARG DEADSNAKES_GPGKEY_URL\nARG GET_PIP_URL\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \\\n        if [ ! -z \"${DEADSNAKES_GPGKEY_URL}\" ] ; then \\\n            mkdir -p -m 0755 /etc/apt/keyrings ; \\\n            curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \\\n            sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \\\n            echo \"deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main\" > /etc/apt/sources.list.d/deadsnakes.list ; \\\n        fi ; \\\n    else \\\n        for i in 1 2 3; do \\\n            add-apt-repository -y ppa:deadsnakes/ppa && break || \\\n            { echo \"Attempt $i failed, retrying in 5s...\"; sleep 5; }; \\\n        done ; \\\n    fi \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\nARG PYTORCH_CUDA_INDEX_BASE_URL\nARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL\nARG PIP_KEYRING_PROVIDER UV_KEYRING_PROVIDER\n\n# Install uv for faster pip installs\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    python3 -m pip install uv\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            \"torch==2.8.0.dev20250318+cu128\" \"torchvision==0.22.0.dev20250319\" ; \\\n        uv pip install --system \\\n            --index-url ${PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \\\n            --pre pytorch_triton==3.3.0+gitab727c40 ; \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system dist/*.whl --verbose \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# If we need to build FlashInfer wheel before its release:\n# $ # Note we remove 7.0 from the arch list compared to the list below, since FlashInfer only supports sm75+\n# $ export TORCH_CUDA_ARCH_LIST='7.5 8.0 8.9 9.0a 10.0a 12.0'\n# $ git clone https://github.com/flashinfer-ai/flashinfer.git --recursive\n# $ cd flashinfer\n# $ git checkout v0.2.6.post1\n# $ python -m flashinfer.aot\n# $ python -m build --no-isolation --wheel\n# $ ls -la dist\n# -rw-rw-r-- 1 mgoin mgoin 205M Jun  9 18:03 flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n# $ # upload the wheel to a public location, e.g. https://wheels.vllm.ai/flashinfer/v0.2.6.post1/flashinfer_python-0.2.6.post1-cp39-abi3-linux_x86_64.whl\n\n# Install FlashInfer from source\nARG FLASHINFER_GIT_REPO=\"https://github.com/flashinfer-ai/flashinfer.git\"\n# Keep this in sync with https://github.com/vllm-project/vllm/blob/main/requirements/cuda.txt\n# We use `--force-reinstall --no-deps` to avoid issues with the existing FlashInfer wheel.\nARG FLASHINFER_GIT_REF=\"v0.2.9rc2\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    git clone --depth 1 --recursive --shallow-submodules \\\n        --branch ${FLASHINFER_GIT_REF} \\\n        ${FLASHINFER_GIT_REPO} flashinfer\n    # Exclude CUDA arches for older versions (11.x and 12.0-12.7)\n    # TODO: Update this to allow setting TORCH_CUDA_ARCH_LIST as a build arg.\n    if [[ \"${CUDA_VERSION}\" == 11.* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9\"\n    elif [[ \"${CUDA_VERSION}\" == 12.[0-7]* ]]; then\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a\"\n    else\n        # CUDA 12.8+ supports 10.0a and 12.0\n        FI_TORCH_CUDA_ARCH_LIST=\"7.5 8.0 8.9 9.0a 10.0a 12.0\"\n    fi\n    echo \"\ud83c\udfd7\ufe0f  Building FlashInfer for arches: ${FI_TORCH_CUDA_ARCH_LIST}\"\n    # Needed to build AOT kernels\n    pushd flashinfer\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            python3 -m flashinfer.aot\n        TORCH_CUDA_ARCH_LIST=\"${FI_TORCH_CUDA_ARCH_LIST}\" \\\n            uv pip install --system --no-build-isolation --force-reinstall --no-deps .\n    popd\n    rm -rf flashinfer\nBASH\nCOPY examples examples\nCOPY benchmarks benchmarks\nCOPY ./vllm/collect_env.py .\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n. /etc/environment && \\\nuv pip list\n\n# Even when we build Flashinfer with AOT mode, there's still\n# some issues w.r.t. JIT compilation. Therefore we need to\n# install build dependencies for JIT compilation.\n# TODO: Remove this once FlashInfer AOT wheel is fixed\nCOPY requirements/build.txt requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -r requirements/build.txt \\\n        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')\n\n# Install DeepGEMM from source\nARG DEEPGEMM_GIT_REPO=\"https://github.com/deepseek-ai/DeepGEMM.git\"\nARG DEEPGEMM_GIT_REF=\"187656694f7f69e3e7975617a68bc3387680a7e1\"\nRUN --mount=type=cache,target=/root/.cache/uv bash - <<'BASH'\n  . /etc/environment\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"\n    CUDA_MINOR=\"${CUDA_VERSION#${CUDA_MAJOR}.}\"\n    CUDA_MINOR=\"${CUDA_MINOR%%.*}\"\n    if [ \"$CUDA_MAJOR\" -ge 12 ] && [ \"$CUDA_MINOR\" -ge 8 ]; then\n        git clone --recursive --shallow-submodules \\\n            ${DEEPGEMM_GIT_REPO} deepgemm\n        echo \"\ud83c\udfd7\ufe0f  Building DeepGEMM\"\n        pushd deepgemm\n            git checkout ${DEEPGEMM_GIT_REF}\n            # Build DeepGEMM\n            # (Based on https://github.com/deepseek-ai/DeepGEMM/blob/main/install.sh)\n            rm -rf build dist\n            rm -rf *.egg-info\n            python3 setup.py bdist_wheel\n            uv pip install --system dist/*.whl\n        popd\n        rm -rf deepgemm\n    else\n        echo \"Skipping DeepGEMM installation (requires CUDA 12.8+ but got ${CUDA_VERSION})\"\n    fi\nBASH\n\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\nARG PYTHON_VERSION\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\nENV UV_INDEX_STRATEGY=\"unsafe-best-match\"\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    CUDA_MAJOR=\"${CUDA_VERSION%%.*}\"; \\\n    if [ \"$CUDA_MAJOR\" -ge 12 ]; then \\\n        uv pip install --system -r requirements/dev.txt; \\\n    fi\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv pip install --system hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python${PYTHON_VERSION}/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN cp -r examples test_docs/\nRUN mv vllm test_docs/\nRUN mv mkdocs.yaml test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\nARG TARGETPLATFORM\nARG INSTALL_KV_CONNECTORS=false\n\nARG PIP_INDEX_URL UV_INDEX_URL\nARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL\n\n# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out\n# Reference: https://github.com/astral-sh/uv/pull/1694\nENV UV_HTTP_TIMEOUT=500\n\nCOPY requirements/kv_connectors.txt requirements/kv_connectors.txt\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    if [ \"$INSTALL_KV_CONNECTORS\" = \"true\" ]; then \\\n        uv pip install --system -r requirements/kv_connectors.txt; \\\n    fi; \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        BITSANDBYTES_VERSION=\"0.42.0\"; \\\n    else \\\n        BITSANDBYTES_VERSION=\"0.46.1\"; \\\n    fi; \\\n    uv pip install --system accelerate hf_transfer modelscope \"bitsandbytes>=${BITSANDBYTES_VERSION}\" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"}
